{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on these:\n",
    "https://medium.com/geekculture/fine-tuning-bert-with-5-lines-of-code-78803a51c14b\n",
    "https://huggingface.co/docs/transformers/training\n",
    "\n",
    "https://discuss.huggingface.co/t/i-have-trained-my-classifier-now-how-do-i-do-predictions/3625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "information about tge ct-bert is here:\n",
    "https://github.com/digitalepidemiologylab/covid-twitter-bert\n",
    "https://huggingface.co/digitalepidemiologylab/covid-twitter-bert\n",
    "finetuning seems complicated.\n",
    "\n",
    "conventional bert models could do the trick, the results may not be that good tho. Uncomment the right cells to use the intended model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_excel('tweets_labeled.xlsx')\n",
    "tweets = d['full_text'][:600]\n",
    "labels = d['Label (critical: 1/neutral: 0)'][:600]\n",
    "data = pd.DataFrame({'tweets': tweets, 'labels': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test = train_test_split(data,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"digitalepidemiologylab/covid-twitter-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize function\n",
    "def tokenize_text(data):\n",
    "    encoded = tokenizer(data, padding=True, truncation=True, return_tensors='np')\n",
    "    return encoded.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenize_text(list(data_train['tweets']))\n",
    "test_data = tokenize_text(list(data_test['tweets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" for i in range(len(train_data['attention_mask'])):\\n    for j in range(len(train_data['attention_mask'][i])):\\n        if train_data['attention_mask'][i][j] == 0:\\n            if train_data['input_ids'][i][j] != 0:\\n                print(train_data['input_ids'][i][j]) \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" for i in range(len(train_data['attention_mask'])):\n",
    "    for j in range(len(train_data['attention_mask'][i])):\n",
    "        if train_data['attention_mask'][i][j] == 0:\n",
    "            if train_data['input_ids'][i][j] != 0:\n",
    "                print(train_data['input_ids'][i][j]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_transform', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'pre_classifier', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = transformers.AutoModel.from_pretrained(\"digitalepidemiologylab/covid-twitter-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(data_train['labels']).astype(np.int64)\n",
    "test_labels = np.array(data_test['labels']).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x216757cd160>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x216757cd160>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.5701 - sparse_categorical_accuracy: 0.7063 WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "15/15 [==============================] - 301s 19s/step - loss: 0.5701 - sparse_categorical_accuracy: 0.7063 - val_loss: 0.6619 - val_sparse_categorical_accuracy: 0.7000\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 311s 21s/step - loss: 0.4654 - sparse_categorical_accuracy: 0.7729 - val_loss: 0.4737 - val_sparse_categorical_accuracy: 0.7667\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 468s 32s/step - loss: 0.4286 - sparse_categorical_accuracy: 0.8313 - val_loss: 0.4588 - val_sparse_categorical_accuracy: 0.8333\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 371s 25s/step - loss: 0.3113 - sparse_categorical_accuracy: 0.8875 - val_loss: 0.3721 - val_sparse_categorical_accuracy: 0.8500\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 353s 24s/step - loss: 0.2185 - sparse_categorical_accuracy: 0.9271 - val_loss: 0.4790 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 368s 25s/step - loss: 0.1424 - sparse_categorical_accuracy: 0.9479 - val_loss: 0.3982 - val_sparse_categorical_accuracy: 0.8167\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 364s 24s/step - loss: 0.0854 - sparse_categorical_accuracy: 0.9708 - val_loss: 0.4516 - val_sparse_categorical_accuracy: 0.8417\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 246s 16s/step - loss: 0.0542 - sparse_categorical_accuracy: 0.9833 - val_loss: 0.5632 - val_sparse_categorical_accuracy: 0.8333\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 243s 16s/step - loss: 0.0366 - sparse_categorical_accuracy: 0.9854 - val_loss: 0.5622 - val_sparse_categorical_accuracy: 0.8083\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 256s 17s/step - loss: 0.0332 - sparse_categorical_accuracy: 0.9854 - val_loss: 0.6702 - val_sparse_categorical_accuracy: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2162260b2e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=tf.metrics.SparseCategoricalAccuracy())\n",
    "model.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    validation_data=(test_data, test_labels),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertModel' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\risto\\OneDrive\\Asiakirjat\\masters\\SNLP\\project\\finetune_testing.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/risto/OneDrive/Asiakirjat/masters/SNLP/project/finetune_testing.ipynb#ch0000017?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mpredict(\u001b[39m\"\u001b[39m\u001b[39masdf\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\risto\\OneDrive\\Asiakirjat\\masters\\SNLP\\project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/risto/OneDrive/Asiakirjat/masters/SNLP/project/venv/lib/site-packages/torch/nn/modules/module.py?line=1182'>1183</a>\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   <a href='file:///c%3A/Users/risto/OneDrive/Asiakirjat/masters/SNLP/project/venv/lib/site-packages/torch/nn/modules/module.py?line=1183'>1184</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> <a href='file:///c%3A/Users/risto/OneDrive/Asiakirjat/masters/SNLP/project/venv/lib/site-packages/torch/nn/modules/module.py?line=1184'>1185</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   <a href='file:///c%3A/Users/risto/OneDrive/Asiakirjat/masters/SNLP/project/venv/lib/site-packages/torch/nn/modules/module.py?line=1185'>1186</a>\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'predict'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' pipe = pipeline(task=\\'fill-mask\\', model=model, tokenizer=tokenizer)\\nout = pipe(f\"In places with a lot of people, it\\'s a good idea to wear a {pipe.tokenizer.mask_token}\")\\nprint(json.dumps(out, indent=4)) '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" pipe = pipeline(task='fill-mask', model=model, tokenizer=tokenizer)\n",
    "out = pipe(f\"In places with a lot of people, it's a good idea to wear a {pipe.tokenizer.mask_token}\")\n",
    "print(json.dumps(out, indent=4)) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = TextClassificationPipeline(model = model, tokenizer = tokenizer, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_0', 'score': 0.011910678818821907},\n",
       "  {'label': 'LABEL_1', 'score': 0.988089382648468}]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"RT @EricWishart: Because of #COVID, the mask shop is closed. #HongKong #China https://t.co/FYcJX1pzNK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Covid Death Distribution by County  For ID   2022-03-04:  Latest Covid Insights by Our Analytics Team using USAFacts #datavisualization #datascience #analytics #healthtech #data #covid19 #publichealth #covid #globalhealth #RStats https://t.co/pFJkoMSCwH\n",
      "[[{'label': 'LABEL_0', 'score': 0.9981666803359985}, {'label': 'LABEL_1', 'score': 0.0018332962645217776}]]\n",
      "RT @healthcareontim: Heart disease refers to several types of conditions that affect the heart. https://t.co/FdgGiB8m6E\n",
      "#disease #health #c…\n",
      "[[{'label': 'LABEL_0', 'score': 0.9947262406349182}, {'label': 'LABEL_1', 'score': 0.005273723974823952}]]\n",
      "Heart disease refers to several types of conditions that affect the heart. https://t.co/FdgGiB8m6E\n",
      "#disease #health #coronavirus #virus #corona #covid19 #covid_19 #medicine #love #illness #pandemic #covid #quarantine #stayhome #cancer #life #china #prevention #wellness #healthy\n",
      "[[{'label': 'LABEL_0', 'score': 0.9947708249092102}, {'label': 'LABEL_1', 'score': 0.00522918626666069}]]\n",
      "Article on HR - COVID-19 is Transforming into Learning and Development was covered in tourism Quest\n",
      "\n",
      "https://t.co/j6cAHJ7YP2\n",
      "#lordshotels #lordsresorts #lordsnews #hr #career #covid #tourismquest\n",
      "[[{'label': 'LABEL_0', 'score': 0.9977642297744751}, {'label': 'LABEL_1', 'score': 0.0022357318084686995}]]\n",
      "Plan B - What I Think Will Happen For ...\n",
      "\n",
      "https://t.co/RCMUJUUd3h\n",
      "\n",
      "#2022 #Altcoindaily #Altcoins #BANK #Bestcryptocurrency #Bitboy #BitcoinNews #Bitcoins #Btc #Business #Coinbureau #Covid #Crypto #CryptoNews #Cryptocurrency #CryptocurrencyNews #Cryptonews #Currency https://t.co/6Jezuv5Oys\n",
      "[[{'label': 'LABEL_0', 'score': 0.9902222156524658}, {'label': 'LABEL_1', 'score': 0.009777802973985672}]]\n",
      "COVID19 | India logs 4,362 new cases &amp; 66 deaths in the last 24 hours; Active cases stand at 54,118\n",
      "#covid #covid19 #jkmedia\n",
      "[[{'label': 'LABEL_0', 'score': 0.9974791407585144}, {'label': 'LABEL_1', 'score': 0.0025208881124854088}]]\n",
      "RT @ConorKerley: Scientific research on piperine and COVID-19!?\n",
      "\n",
      "New addition to our blog series:\n",
      "Best supplements and nutraceuticals for C…\n",
      "[[{'label': 'LABEL_0', 'score': 0.9961227774620056}, {'label': 'LABEL_1', 'score': 0.003877239767462015}]]\n",
      "The highly dangerous #COVID virus has infected my youngest child. Here he is suffering in isolation. https://t.co/wzOweynfBg\n",
      "[[{'label': 'LABEL_0', 'score': 0.9625544548034668}, {'label': 'LABEL_1', 'score': 0.03744557872414589}]]\n",
      "RT @donnedia: #SundayMotivation #SundayVibes #SundayThoughts Under Joe Biden\n",
      "*Amer. Is economically stable again\n",
      "*Less Racial tension\n",
      "*NATO…\n",
      "[[{'label': 'LABEL_0', 'score': 0.09320143610239029}, {'label': 'LABEL_1', 'score': 0.9067986011505127}]]\n",
      "RT @ACC_Andrew: When you don't use critical thought &amp; when you don't do your own research. #tyranny #WorldEconomicForum #MediaMafia #Medica…\n",
      "[[{'label': 'LABEL_0', 'score': 0.018018875271081924}, {'label': 'LABEL_1', 'score': 0.981981098651886}]]\n",
      "RT @njoyflyfishing: Total Covid Death Distribution by County  For HI   2022-03-04:  Latest Covid Insights by Our Analytics Team using USAFa…\n",
      "[[{'label': 'LABEL_0', 'score': 0.9982059001922607}, {'label': 'LABEL_1', 'score': 0.0017940497491508722}]]\n",
      "Support us on Buy me a coffee\n",
      "\n",
      "https://t.co/X12iC9rgfF\n",
      "\n",
      "#buymeacoffee #support #love #follow #like #instagram #share #instagood #community #likeforlikes #covid #motivation #help #art #life #comment #supportsmallbusiness #likes  #happy #health #instadaily #charity #trending https://t.co/qPIR32c9cJ\n",
      "[[{'label': 'LABEL_0', 'score': 0.8364691734313965}, {'label': 'LABEL_1', 'score': 0.1635308861732483}]]\n",
      "RT @_lokeshsharma: #Rajasthan #CoronaUpdate:\n",
      "\n",
      "173 #Covid cases reported\n",
      "55 from #Jaipur \n",
      "1 #Corona related death \n",
      "Today's #Recovered✅361\n",
      "Ac…\n",
      "[[{'label': 'LABEL_0', 'score': 0.9980366826057434}, {'label': 'LABEL_1', 'score': 0.001963381189852953}]]\n",
      "@CBSNews CBS is reporting that the CDC says people indoors don’t need to wear masks, which they haven’t been for a while now. #COVID\n",
      "[[{'label': 'LABEL_0', 'score': 0.4200834333896637}, {'label': 'LABEL_1', 'score': 0.5799165964126587}]]\n",
      "India has suggested convening a meeting under the aegis of the #World #Trade Organization (WTO) to discuss the role of e-commerce during the time of the COVID-19 #pandemic.#career #money #success #commerce #help #covid #india #wto #ecommerce #discussions #pandemic #omc #lockdowns https://t.co/L2Ebpi1Ang\n",
      "[[{'label': 'LABEL_0', 'score': 0.9965831637382507}, {'label': 'LABEL_1', 'score': 0.0034169089049100876}]]\n"
     ]
    }
   ],
   "source": [
    "test_tweets = d[\"full_text\"][600:615]\n",
    "for t in test_tweets:\n",
    "    print(t)\n",
    "    print(pipe(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_0', 'score': 0.015024186111986637},\n",
       "  {'label': 'LABEL_1', 'score': 0.9849758744239807}]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Did #VladimirPutin scare #Covid off or what ? It’s literally not on the news at all after dominating our existence over the last few years 🤔 ou’ve gotta be a bit of an ostrich to trust our government these days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save_pretrained('models/tf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/tf_tokenizer\\\\tokenizer_config.json',\n",
       " 'models/tf_tokenizer\\\\special_tokens_map.json',\n",
       " 'models/tf_tokenizer\\\\vocab.txt',\n",
       " 'models/tf_tokenizer\\\\added_tokens.json',\n",
       " 'models/tf_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('models/tf_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_0', 'score': 0.4587249755859375},\n",
       "  {'label': 'LABEL_1', 'score': 0.5412750244140625}]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"we should stop wearing masks\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1699b388bd678768f544dd6534e58979417dc4074b6b5ce767e74c5ad0a1cf40"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
